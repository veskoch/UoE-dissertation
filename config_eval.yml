# ### ### ### ###
# Important

# One of "train_and_eval", "train", "eval", "infer", "export", "score".
run: eval

# Do you want to score and evaluate the train dataset?
# Eval dataset will be evauated either way if `run` involves evaluation
evaluate_train: false

# Logs some prediction time metrics.
log_prediction_time: true

# If set, model_dir will be created relative to this location.
run_dir: trained_models/2018-07-08_1343/run_dir/model_dir

# Custom model configuration file
model: trained_models/2018-07-08_1343/model.py

# Checkpoint or directory to use for inference or export 
# when a directory is set, the latest checkpoint is used
checkpoint_path: trained_models/2018-07-08_1343/run_dir/model_dir

# List of configuration files.
# Accepts multiples so that some parts can be made reusable.
config: [hyperparams_eval.yml]

# ### ### ### ### 
# Other

# Number of GPUs to use for in-graph replication.
num_gpus: 1

# hostname:port of the chief worker (for distributed training).
chief_host:

# Comma-separated list of hostname:port of workers
# for distributed training)."
worker_hosts: 

# Comma-separated list of hostname:port of parameter servers
# (for distributed training).
ps_hosts:

# Type of the task to run (for distributed training).
# One of "chief", "worker", "ps", "evaluator"
task_type: chief

# ID of the task (for distributed training).
task_index: 0

# One of "DEBUG", "ERROR", "FATAL", "INFO", "WARN"
log_level: INFO

# Random seed.
seed: null

# Allocate GPU memory dynamically.
gpu_allow_growth: false

# Number of intra op threads (0 means the system picks
# an appropriate number).
intra_op_parallelism_threads: 0

# Number of inter op threads (0 means the system picks
# an appropriate number).
inter_op_parallelism_threads: 0

# ### ### ### ### 
# Inactive

# File used to save predictions. If not set, predictions are printed
# on the standard output.
predictions_file:

# Run inference on this file.
features_file:

# Model type from the catalog.
model_type: 

# If set, data files are expected to be relative to this location.
data_dir: 