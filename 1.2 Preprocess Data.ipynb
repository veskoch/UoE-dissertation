{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will help you define and run the pipelines for preprocessing your data (which you should have loaded earlier with the help of `1.1. Collate Files.ipynb`).\n",
    "\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "\n",
    "First, adjust the definition of the pipelines inside `pipeline_graph_def`. Then run `build_dataset`. This will create 4 files, two sets of train and evaluate. The first set is the inputs, and the second set is the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magenta.protobuf import music_pb2\n",
    "from magenta.pipelines import pipelines_common, dag_pipeline, note_sequence_pipelines\n",
    "\n",
    "import process_data\n",
    "from process_data import PerformanceExtractor, PerformanceParser\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PARAMETERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = dict()\n",
    "\n",
    "pipeline_config['data_source_dir'] = \"./data/note_seq_proto/\"\n",
    "pipeline_config['data_target_dir'] = \"./data/performance_seq_text/\"\n",
    "\n",
    "pipeline_config['steps_per_second'] = 100\n",
    "\n",
    "pipeline_config['min_events'] = 1\n",
    "pipeline_config['max_events'] = 10000\n",
    "\n",
    "pipeline_config['eval_ratio'] = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEFINITIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_graph_def(collection_name,\n",
    "                       config):\n",
    "    \"\"\"Returns the Pipeline instance which creates the RNN dataset.\n",
    "\n",
    "    Args:\n",
    "        collection_name:\n",
    "        config: dict() with configuration settings\n",
    "\n",
    "    Returns:\n",
    "        A pipeline.Pipeline instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stretch by -5%, -2.5%, 0%, 2.5%, and 5%.\n",
    "    stretch_factors = [0.95, 0.975, 1.0, 1.025, 1.05]\n",
    "\n",
    "    # Transpose no more than a major third.\n",
    "    transposition_range = range(-3, 4)\n",
    "\n",
    "    partitioner = pipelines_common.RandomPartition(\n",
    "        music_pb2.NoteSequence,\n",
    "        ['eval_arrangement' + '_' + collection_name,  \n",
    "         'train_arrangement' + '_' + collection_name],\n",
    "        [pipeline_config['eval_ratio']])\n",
    "    dag = {partitioner: dag_pipeline.DagInput(music_pb2.NoteSequence)}\n",
    "\n",
    "    for mode in ['eval', 'train']:\n",
    "        key = mode + '_arrangement' + '_' + collection_name\n",
    "        \n",
    "        quantizer = note_sequence_pipelines.Quantizer(\n",
    "            steps_per_second=pipeline_config['steps_per_second'], \n",
    "            name='Quantizer_' + key)\n",
    "        \n",
    "        perf_extractor = PerformanceExtractor(\n",
    "            min_events=pipeline_config['min_events'],\n",
    "            max_events=pipeline_config['max_events'],\n",
    "            num_velocity_bins=0,\n",
    "            name='PerformanceExtractor_' + key)\n",
    "            # input_type = music_pb2.NoteSequence\n",
    "            # output_type = magenta.music.Performance\n",
    "            \n",
    "        perf_parser = PerformanceParser(\n",
    "            name='PerformanceParser_' + key)\n",
    "            # input_type = magenta.music.Performance\n",
    "            # output_type = str\n",
    "        \n",
    "        dag[quantizer] = partitioner[key]\n",
    "        dag[perf_extractor] = quantizer\n",
    "        dag[perf_parser] = perf_extractor\n",
    "        dag[dag_pipeline.DagOutput(key)] = perf_parser\n",
    "        \n",
    "    return dag_pipeline.DAGPipeline(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "File ./data/performance_seq_text/eval_arrangement_inputs.txt already exists. Please remove and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1edd0821a304>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/GitHub/UoE-dissertation/process_data.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(pipeline_config, pipeline_graph_def)\u001b[0m\n\u001b[1;32m    151\u001b[0m             pipeline.tf_record_iterator(src_file,\n\u001b[1;32m    152\u001b[0m                                         pipeline_graph.input_type),\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0moutput_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/UoE-dissertation/process_data.py\u001b[0m in \u001b[0;36mrun_pipeline_text\u001b[0;34m(pipeline, input_iterator, output_dir)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             raise FileExistsError('File {} already exists. Please remove and try again.'\n\u001b[0;32m--> 108\u001b[0;31m                                         .format(path))           \n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     writers = dict([(name, open(path, 'a'))\n",
      "\u001b[0;31mFileExistsError\u001b[0m: File ./data/performance_seq_text/eval_arrangement_inputs.txt already exists. Please remove and try again."
     ]
    }
   ],
   "source": [
    "process_data.build_dataset(pipeline_config, pipeline_graph_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: File ./data/performance_seq_text/vocab.txt exists. Removing. Rebuilding vocabulary.\n",
      "INFO: Vocabulary built.\n"
     ]
    }
   ],
   "source": [
    "process_data.build_vocab(pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
